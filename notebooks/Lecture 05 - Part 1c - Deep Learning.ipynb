{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79086f30-7667-4494-8b44-b1f460011637",
   "metadata": {},
   "source": [
    "# **SIN 393 – Introduction to Computer Vision (2023)**\n",
    "\n",
    "# Lecture 05 - Part 1c - Deep Learning\n",
    "\n",
    "Prof. João Fernando Mari ([*joaofmari.github.io*](https://joaofmari.github.io/))\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3df9732-cb78-4dfe-a2e3-ae6f351096fb",
   "metadata": {},
   "source": [
    "## Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "461fbf94-6c64-4ce6-a144-d82380b68017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93f4ec0-39ee-4de3-8fa2-edb9ceac323b",
   "metadata": {},
   "source": [
    "## Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c109a8cc-5591-4bd3-ba41-a8d238fd3d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(v):\n",
    "    return 1 / (1 + np.exp(-v))\n",
    "    ### return np.array([1 / (1 + math.exp(-v[0])), 1 / (1 + math.exp(-v[1]))])\n",
    "\n",
    "def sigmoid_grad(v):\n",
    "    y_ = sigmoid(v) * (1 - sigmoid(v))\n",
    "    return y_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd31821-9d46-4624-b9c2-1b5881437e08",
   "metadata": {},
   "source": [
    "## Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbffa75d-976d-4129-9e4e-3b3be46a524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_mse(y, y_hat):\n",
    "    return (y - y_hat)**2\n",
    "\n",
    "def loss_mse_grad(y, y_hat):\n",
    "    return -1 * (y - y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae36e65-d3e4-4af8-9c47-28a9fa419fb9",
   "metadata": {},
   "source": [
    "## Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fff308ac-f39e-4094-9b86-42901a478e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trunc(values, decs=0):\n",
    "    return np.trunc(values*10**decs)/(10**decs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deff2ad7-712d-4512-bb74-418a61d797cc",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef7b8ad0-8493-4962-be14-1281649c9cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.01\n",
    "\n",
    "DEC = 4 # PyTorch ans Matlab default is 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae4b5a4-6c78-448b-a569-3b9412c3ef68",
   "metadata": {},
   "source": [
    "## Inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4067b93b-3ee0-488d-b5f1-12a48e0fa1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0.3])\n",
    "y = np.array([1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588ef46a-a760-46d3-88fa-a65c130dcaa3",
   "metadata": {},
   "source": [
    "## Weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "205984ed-4cdb-4149-879f-25973a7f8e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = np.array([[0.1]])\n",
    "\n",
    "b0 = np.array([0.25])\n",
    "\n",
    "w1 = np.array([[0.5]])\n",
    "\n",
    "b1 = np.array([0.35])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0a44fe-4860-4c3e-828d-4609d6353812",
   "metadata": {},
   "source": [
    "## Interactive parameter tunning\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dc03798-1287-4ece-9b05-55d05179ed59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(x, y, w0, b0, w1, b1):\n",
    "    # FORWARD\n",
    "    # ==================================================\n",
    "    v0 = np.dot(x, w0) + b0\n",
    "    ### print(f'v0 = {np.around(v0, DEC)}')\n",
    "    \n",
    "    y0 = sigmoid(v0)\n",
    "    ### print(f'y0 = {np.around(y0, DEC)}')\n",
    "    \n",
    "    v1 = np.dot(y0, w1) + b1\n",
    "    ### print(f'v1 = {np.around(v1, DEC)}')\n",
    "    \n",
    "    y_hat = sigmoid(v1)\n",
    "    ### print(f'y_hat = {np.around(y_hat, DEC)}')\n",
    "\n",
    "    # Loss\n",
    "    L = loss_mse(y, y_hat).mean()\n",
    "    ### print(f'L = {np.around(L, DEC)}')\n",
    "\n",
    "    # # BACKPROPAGATION\n",
    "    # # ==================================================\n",
    "    # # Layer 1\n",
    "    # # --------------------------------------------------\n",
    "    # # ∂L/∂y^ \n",
    "    # dL_dyhat = loss_mse_grad(y, y_hat)\n",
    "    # ### print(f'∂L/∂y^ = {np.around(dL_dyhat, DEC)}')\n",
    "    \n",
    "    # # ∂y^/∂v1\n",
    "    # dyhat_dv1 = sigmoid_grad(v1)\n",
    "    # ### print(f'\\n∂y^/∂v1 = {np.around(dyhat_dv1, DEC)}')\n",
    "    \n",
    "    # # ∂v1/∂W1\n",
    "    # dv1_dW1 = np.hstack([y0[np.newaxis].T] * len(dyhat_dv1))\n",
    "    # ### print(f'\\n∂y^/∂W1 = \\n{np.around(dv1_dW1, DEC)}')\n",
    "    \n",
    "    # # ∂L/∂W1 = ∂L/∂y^ * ∂y^/∂v1 * ∂v1/∂W1\n",
    "    # # -----------------------------------\n",
    "    # dL_dW1 = dL_dyhat * dyhat_dv1 * dv1_dW1\n",
    "    # ### print(f'\\n∂L/∂W1 = \\n{np.around(dL_dW1, DEC)}')\n",
    "\n",
    "    # # ∂v1/∂b1\n",
    "    # # As the input for bias is fixed in 1, the derivatives are 1.\n",
    "    \n",
    "    # # ∂L/∂b1 = ∂L/∂y^ * ∂y^/∂v1 * ∂v1/∂b1\n",
    "    # # -----------------------------------\n",
    "    # dL_db1 = dL_dyhat * dyhat_dv1 \n",
    "    # ### print(f'∂L/∂b1 = \\n{np.around(dL_db1, DEC)}')\n",
    "\n",
    "    # # Layer 0\n",
    "    # # --------------------------------------------------\n",
    "    # # ∂v1/∂y0\n",
    "    # dv1_dy0 = w1\n",
    "    \n",
    "    # # ∂L/∂y0 = ∂L/∂y^ * ∂y^/∂v1 * ∂v1/∂y0\n",
    "    # # -----------------------------------\n",
    "    # dL_dy0_ = dL_dyhat * dyhat_dv1 * dv1_dy0\n",
    "    # ### print(f'∂L/∂y0 * = \\n{np.around(dL_dy0_, DEC)}')\n",
    "    \n",
    "    # # Summing the contributions of layer 1\n",
    "    # dL_dy0 = dL_dy0_.sum(axis=1)\n",
    "    # ### print(f'\\n∂L/∂y01 = {np.around(dL_dy0, DEC)}')\n",
    "\n",
    "    # # ∂y0/∂v0\n",
    "    # dy0_dv0 = sigmoid_grad(v0)\n",
    "    # ### print(f'∂y0/∂v0 = {dy0_dv0}')\n",
    "    \n",
    "    # # ∂v0/∂W0\n",
    "    # dv0_dW0 = np.hstack([x[np.newaxis].T] * len(dy0_dv0))\n",
    "    # ### print(f'\\n∂v0/∂W0 = \\n{np.around(dv0_dW0, DEC)}')\n",
    "    \n",
    "    # # ∂L/∂W0 = ∂L/∂y0 * ∂y0/∂v0 * ∂v0/∂yW0\n",
    "    # # ------------------------------------\n",
    "    # dL_dW0 = dL_dy0 * dy0_dv0 * dv0_dW0\n",
    "    # ### print(f'\\n∂L/∂W0 = \\n{np.around(dL_dW0, DEC)}')\n",
    "\n",
    "    # # * ∂v1/∂b1 = 1\n",
    "    # # As the input for bias is fixed in 1, the derivatives are 1.\n",
    "    \n",
    "    # # ∂L/∂b0 = ∂L/∂y0 * ∂y0/∂v0 \n",
    "    # # -----------------------------------\n",
    "    # dL_db0 = dL_dy0 * dy0_dv0 \n",
    "    # ### print(f'\\n∂L/∂b0 = \\n{np.around(dL_db0, DEC)}')    \n",
    "\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b47de7f5-9611-436c-b3be-185281ba778f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computer_loss_space(x, y, w0_range, b0_range, w1_range, b1_range):\n",
    "\n",
    "    loss_space = np.zeros([len(w0_range), len(b0_range), len(w1_range), len(b1_range)])\n",
    "\n",
    "    for i, w0_ in enumerate(w0_range):\n",
    "        for j, b0_ in enumerate(b0_range):\n",
    "            for k, w1_ in enumerate(w1_range):\n",
    "                for l, b1_ in enumerate(b1_range):\n",
    "                    loss = step(x, y, np.array([[w0_]]), np.array([[b0_]]), np.array([[w1_]]), np.array([[b1_]]))\n",
    "                    loss_space[i, j, k, l] = loss    \n",
    "\n",
    "    return loss_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1d9e1ba-9340-43d3-a831-dcd47ee49c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive(loss_space, w0, b0, w1, b1, w0_r, b0_r, w1_r, b1_r):\n",
    "\n",
    "    w0_v = w0_r[w0]\n",
    "    b0_v = b0_r[b0]\n",
    "    w1_v = w1_r[w1]\n",
    "    b1_v = b1_r[b1]\n",
    "\n",
    "    print(f'Loss = {loss_space[w0,b0,w1,b1]}, w0 = {w0}, b0 = {b0}, w1 = {w1}, b1 = {b1}')\n",
    "\n",
    "    fig, ax  = plt.subplots(1, 4, figsize=(12, 3))\n",
    "\n",
    "    ax[0].plot(w0_r, loss_space[:,b0,w1,b1])\n",
    "    ax[0].axvline(w0_v, color='r')\n",
    "    ax[0].set_title('$w_0$')\n",
    "    ax[0].set_xlabel('$w_0$')\n",
    "    ax[0].set_ylabel('$Error$')\n",
    "    \n",
    "    ax[1].plot(b0_r, loss_space[w0,:,w1,b1])\n",
    "    ax[1].axvline(b0_v, color='r')\n",
    "    ax[1].set_title('$b_0$')\n",
    "    ax[1].set_xlabel('$b_0$')\n",
    "    ### ax[1].set_ylabel('$Error$')\n",
    "    \n",
    "    ax[2].plot(w1_r, loss_space[w0,b0,:,b1])\n",
    "    ax[2].axvline(w1_v, color='r')\n",
    "    ax[2].set_title('$w_1$')\n",
    "    ax[2].set_xlabel('$w_1$')\n",
    "    ### ax[2].set_ylabel('$Error$')\n",
    "    \n",
    "    ax[3].plot(b1_r, loss_space[w0,b0,w1,:])   \n",
    "    ax[3].axvline(b1_v, color='r')\n",
    "    ax[3].set_title('$b_1$')\n",
    "    ax[3].set_xlabel('$b_1$')\n",
    "    ### ax[3].set_ylabel('$Error$')\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3eb60ab-eeb6-4962-9e02-7aefc4aa4fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "w0_range = np.arange(-4, 4, 0.2)\n",
    "w1_range = np.arange(-4, 4, 0.2)\n",
    "b0_range = np.arange(-4, 4, 0.2)\n",
    "b1_range = np.arange(-4, 4, 0.2)\n",
    "\n",
    "loss_space = computer_loss_space(x, y, w0_range, b0_range, w1_range, b1_range)\n",
    "### print(loss_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e95163d-1f0b-4489-88ca-a449407aac75",
   "metadata": {},
   "source": [
    "![title](figures/nn01a_ok.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a6f6db9-dfc5-47f2-9313-fe3ef5e355fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d0958b5d7540628562a745d07dc690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=20, description='w0', max=39), IntSlider(value=20, description='b0', max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.interactive(loss_space, w0, b0, w1, b1, w0_r, b0_r, w1_r, b1_r)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slider_w0 = widgets.IntSlider(value=20, min=0, max=39)\n",
    "slider_b0 = widgets.IntSlider(value=20, min=0, max=39)\n",
    "slider_w1 = widgets.IntSlider(value=20, min=0, max=39)\n",
    "slider_b1 = widgets.IntSlider(value=20, min=0, max=39)\n",
    "\n",
    "widgets.interact(interactive, loss_space=widgets.fixed(loss_space), \n",
    "                 w0=slider_w0,  b0=slider_b0, w1=slider_w1, b1=slider_b1, \n",
    "                 w0_r=widgets.fixed(w0_range), b0_r=widgets.fixed(b0_range), w1_r=widgets.fixed(w1_range), b1_r=widgets.fixed(b1_range))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1351ea-9b55-42db-890c-7a1183699cd6",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "---\n",
    "* Rabindra Lamsal. A step by step forward pass and backpropagation example\n",
    "    * https://theneuralblog.com/forward-pass-backpropagation-example/\n",
    "* Matt Mazur. A Step by Step Backpropagation Example\n",
    "    * https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\n",
    "* Back-Propagation is very simple. Who made it Complicated ?\n",
    "    * https://medium.com/@14prakash/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c \n",
    "* Chapter 7: Artificial neural networks with Math.\n",
    "    * https://medium.com/deep-math-machine-learning-ai/chapter-7-artificial-neural-networks-with-math-bb711169481b\n",
    "* The Matrix Calculus You Need For Deep Learning\n",
    "    * http://explained.ai/matrix-calculus/index.html \n",
    "* How backpropagation works, and how you can use Python to build a neural network\n",
    "    * https://medium.freecodecamp.org/build-a-flexible-neural-network-with-backpropagation-in-python-acffeb7846d0 \n",
    "* All the Backpropagation derivatives\n",
    "    * https://medium.com/@pdquant/all-the-backpropagation-derivatives-d5275f727f60\n",
    "* Brent Scarff. Understanding Backpropagation. \n",
    "    * https://towardsdatascience.com/understanding-backpropagation-abcc509ca9d0 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
