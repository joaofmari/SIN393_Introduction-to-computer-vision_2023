{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79086f30-7667-4494-8b44-b1f460011637",
   "metadata": {},
   "source": [
    "# **SIN 393 – Introduction to Computer Vision (2023)**\n",
    "\n",
    "# Lecture 05 - Part 2a - Deep Learning\n",
    "\n",
    "Prof. João Fernando Mari ([*joaofmari.github.io*](https://joaofmari.github.io/))\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3df9732-cb78-4dfe-a2e3-ae6f351096fb",
   "metadata": {},
   "source": [
    "## Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "461fbf94-6c64-4ce6-a144-d82380b68017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93f4ec0-39ee-4de3-8fa2-edb9ceac323b",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c109a8cc-5591-4bd3-ba41-a8d238fd3d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(v):\n",
    "    return 1 / (1 + np.exp(-v))\n",
    "    ### return np.array([1 / (1 + math.exp(-v[0])), 1 / (1 + math.exp(-v[1]))])\n",
    "\n",
    "def sigmoid_grad(v):\n",
    "    y_ = sigmoid(v) * (1 - sigmoid(v))\n",
    "    return y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "781cff80-a080-4857-bd15-763f48a1e103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(v):\n",
    "    return np.maximum(0, v)\n",
    "\n",
    "def relu_grad(v):\n",
    "    return np.greater(v, 0).astype(float)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd31821-9d46-4624-b9c2-1b5881437e08",
   "metadata": {},
   "source": [
    "## Loss functions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbffa75d-976d-4129-9e4e-3b3be46a524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_mse(y, y_hat):\n",
    "    ### return (y - y_hat)**2\n",
    "    return (1/len(y)) * (y - y_hat)**2\n",
    "\n",
    "def loss_mse_grad(y, y_hat):\n",
    "    N = len(y)\n",
    "    return -(2 / N) * (y - y_hat)\n",
    "    ### return -1 *  (y - y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca04f9ab-be1e-4e66-820d-3f1cf8256f54",
   "metadata": {},
   "source": [
    "## Architecture and hyperparameters\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae4b5a4-6c78-448b-a569-3b9412c3ef68",
   "metadata": {},
   "source": [
    "### Inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4067b93b-3ee0-488d-b5f1-12a48e0fa1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0.3]])\n",
    "y = np.array([1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588ef46a-a760-46d3-88fa-a65c130dcaa3",
   "metadata": {},
   "source": [
    "### Weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "205984ed-4cdb-4149-879f-25973a7f8e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "W0 = np.array([[0.1, 0.2]])\n",
    "\n",
    "b0 = np.array([0.25, 0.25])\n",
    "\n",
    "W1 = np.array([[0.5],\n",
    "               [0.6]])\n",
    "\n",
    "b1 = np.array([0.35])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05510b4-29bf-42dd-9792-c4e6ffa0c891",
   "metadata": {},
   "source": [
    "![title](figures/nn02_ok.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deff2ad7-712d-4512-bb74-418a61d797cc",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef7b8ad0-8493-4962-be14-1281649c9cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.01\n",
    "\n",
    "# For float numbers rounding\n",
    "DEC = 4 # PyTorch and Matlab default is 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0a44fe-4860-4c3e-828d-4609d6353812",
   "metadata": {},
   "source": [
    "## Forward pass\n",
    "---\n",
    "\n",
    "$\\mathbf{v}^0 = \\mathbf{x}\\mathbf{W}^{0} + \\mathbf{b}^0$\n",
    "\n",
    "$\\mathbf{y}^0 = \\sigma(\\mathbf{v}^0)$\n",
    "\n",
    "$\\mathbf{v}^{1} = \\mathbf{y}^{0}\\mathbf{W}^{1} + \\mathbf{b}^{1}$\n",
    "\n",
    "$\\mathbf{\\hat{y}} = \\sigma(\\mathbf{v}^1)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b47de7f5-9611-436c-b3be-185281ba778f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v0 = [[0.28 0.31]]\n",
      "y0 = [[0.5695 0.5769]]\n",
      "v1 = [[0.9809]]\n",
      "y_hat = [[0.7273]]\n"
     ]
    }
   ],
   "source": [
    "v0 = np.dot(x, W0) + b0\n",
    "print(f'v0 = {np.around(v0, DEC)}')\n",
    "\n",
    "y0 = sigmoid(v0)\n",
    "print(f'y0 = {np.around(y0, DEC)}')\n",
    "\n",
    "v1 = np.dot(y0, W1) + b1\n",
    "print(f'v1 = {np.around(v1, DEC)}')\n",
    "\n",
    "y_hat = sigmoid(v1)\n",
    "print(f'y_hat = {np.around(y_hat, DEC)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3eb60ab-eeb6-4962-9e02-7aefc4aa4fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L = [[0.0744]]\n"
     ]
    }
   ],
   "source": [
    "### L = loss_mse(y, y_hat).mean()\n",
    "L = loss_mse(y, y_hat)\n",
    "print(f'L = {np.around(L, DEC)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00983bf-a4d5-48c1-bfbf-7852b1e3014b",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026bcbe5-5506-479d-aa5a-ade90bd712ac",
   "metadata": {},
   "source": [
    "### Layer 1\n",
    "\n",
    "* We need to find $\\frac{\\partial{L}}{\\partial{\\mathbf{W}^1}}$ to update the weights, $\\mathbf{W}^1$, through Gradient Descent.\n",
    "* We can compute $\\frac{\\partial{L}}{\\partial{\\mathbf{W}^1}}$ using the chain rule:\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{\\mathbf{W^1}}} = \\frac{\\partial{L}}{\\partial{\\mathbf{\\hat{y}}}} \\times \\frac{\\partial{\\mathbf{\\hat{y}}}}{\\partial{\\mathbf{v^1}}} \\times \\frac{\\partial{\\mathbf{v^1}}}{\\partial{\\mathbf{W^1}}}$$\n",
    "\n",
    "* We also need to find $\\frac{\\partial{L}}{\\partial{\\mathbf{b^1}}}$ to update the bias, $\\mathbf{b}^1$, through Gradient Descent.\n",
    "* We can compute $\\frac{\\partial{L}}{\\partial{\\mathbf{b^1}}}$ using the chain rule:\n",
    "$$\\frac{\\partial{L}}{\\partial{\\mathbf{b^1}}} = \\frac{\\partial{L}}{\\partial{\\mathbf{\\hat{y}}}} \\times \\frac{\\partial{\\mathbf{\\hat{y}}}}{\\partial{\\mathbf{v^1}}} \\times \\frac{\\partial{\\mathbf{v^1}}}{\\partial{\\mathbf{b^1}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bf812c-010b-4448-89c6-8071c92915c3",
   "metadata": {},
   "source": [
    "#### Weights\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{\\mathbf{W^1}}} = \\frac{\\partial{L}}{\\partial{\\mathbf{\\hat{y}}}} \\times \\frac{\\partial{\\mathbf{\\hat{y}}}}{\\partial{\\mathbf{v^1}}} \\times \\frac{\\partial{\\mathbf{v^1}}}{\\partial{\\mathbf{W^1}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cbcc422-5292-4e0d-96bd-b7bc7362daf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "∂L/∂y^ = [[-0.5454]]\n",
      "\n",
      "∂y^/∂v1 = [[0.1983]]\n",
      "\n",
      "∂y^/∂W1 = \n",
      "[[[0.5695]]\n",
      "\n",
      " [[0.5769]]]\n",
      "\n",
      "∂L/∂W1 = \n",
      "[[[-0.0616]]\n",
      "\n",
      " [[-0.0624]]]\n"
     ]
    }
   ],
   "source": [
    "# ∂L/∂y^ \n",
    "dL_dyhat = loss_mse_grad(y, y_hat)\n",
    "print(f'∂L/∂y^ = {np.around(dL_dyhat, DEC)}')\n",
    "\n",
    "# ∂y^/∂v1\n",
    "dyhat_dv1 = sigmoid_grad(v1)\n",
    "print(f'\\n∂y^/∂v1 = {np.around(dyhat_dv1, DEC)}')\n",
    "\n",
    "# ∂v1/∂W1\n",
    "dv1_dW1 = np.hstack([y0[np.newaxis].T] * len(dyhat_dv1))\n",
    "print(f'\\n∂y^/∂W1 = \\n{np.around(dv1_dW1, DEC)}')\n",
    "\n",
    "# ∂L/∂W1 = ∂L/∂y^ * ∂y^/∂v1 * ∂v1/∂W1\n",
    "# -----------------------------------\n",
    "dL_dW1 = dL_dyhat * dyhat_dv1 * dv1_dW1\n",
    "print(f'\\n∂L/∂W1 = \\n{np.around(dL_dW1, DEC)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7188cdae-42e2-47a9-ae43-d21182fe1ec8",
   "metadata": {},
   "source": [
    "#### Bias\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{\\mathbf{b^1}}} = \\frac{\\partial{L}}{\\partial{\\mathbf{\\hat{y}}}} \\times \\frac{\\partial{\\mathbf{\\hat{y}}}}{\\partial{\\mathbf{v^1}}} \\times \\frac{\\partial{\\mathbf{v^1}}}{\\partial{\\mathbf{b^1}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6129e7ea-3a81-4bb3-94e2-29ecb1c56c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "∂L/∂b1 = [[-0.1082]]\n"
     ]
    }
   ],
   "source": [
    "# ∂v1/∂b1\n",
    "# As the input for bias is fixed in 1, the derivatives are 1.\n",
    "\n",
    "# ∂L/∂b1 = ∂L/∂y^ * ∂y^/∂v1 * ∂v1/∂b1\n",
    "# -----------------------------------\n",
    "dL_db1 = dL_dyhat * dyhat_dv1 \n",
    "print(f'∂L/∂b1 = {np.around(dL_db1, DEC)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d67075-5c5d-47a6-aa28-20e0cc08ce6e",
   "metadata": {},
   "source": [
    "### Layer 0\n",
    "\n",
    "* We need to find $\\frac{\\partial{L}}{\\partial{\\mathbf{W}^0}}$ to update the weights, $\\mathbf{W}^0$ through Gradient Descent.\n",
    "* We can compute $\\frac{\\partial{L}}{\\partial{\\mathbf{W}^0}}$ using the chain rule:\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{\\mathbf{W}^0}} = \\frac{\\partial{L}}{\\partial{\\mathbf{\\hat{y}}}} \\times \\frac{\\partial{\\mathbf{\\hat{y}}}}{\\partial{\\mathbf{v^1}}} \\times \\frac{\\partial{\\mathbf{v^1}}}{\\partial{\\mathbf{y^0}}} \\times \\frac{\\partial{\\mathbf{y^0}}}{\\partial{\\mathbf{v^0}}} \\times \\frac{\\partial{\\mathbf{v^0}}}{\\partial{\\mathbf{W}^0}}$$\n",
    "\n",
    "* Simplifying to use the already calculated values:\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{\\mathbf{W}^0}} = \\frac{\\partial{L}}{\\partial{\\mathbf{y^0}}} \\times \\frac{\\partial{\\mathbf{y^0}}}{\\partial\\mathbf{{v^0}}} \\times \\frac{\\partial{\\mathbf{v^0}}}{\\partial{\\mathbf{W}^0}}$$\n",
    "\n",
    "* where:\n",
    "$$\\frac{\\partial{L}}{\\partial{\\mathbf{y^0}}} = \\frac{\\partial{L}}{\\partial{\\mathbf{\\hat{y}}}} \\times \\frac{\\partial{\\mathbf{\\hat{y}}}}{\\partial{\\mathbf{v^1}}} \\times \\frac{\\partial{\\mathbf{v^1}}}{\\partial{\\mathbf{y^0}}}$$\n",
    "\n",
    "* in which $\\frac{\\partial{L}}{\\partial{\\mathbf{\\hat{y}}}}$ and $\\frac{\\partial{\\mathbf{\\hat{y}}}}{\\partial{\\mathbf{v^1}}}$ has already been calculated, and:\n",
    "\n",
    "$$\\frac{\\partial{\\mathbf{v}^1}}{\\partial{\\mathbf{y}^0}} = \\mathbf{W}^1$$\n",
    "\n",
    "* We need to find $\\frac{\\partial{L}}{\\partial{\\mathbf{b}^0}}$ to update the bias, $\\mathbf{b}^0$, through Gradient Descent.\n",
    "* We can compute $\\frac{\\partial{L}}{\\partial{\\mathbf{b}^0}}$ using the chain rule:\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{\\mathbf{b}^0}} = \\frac{\\partial{L}}{\\partial{\\mathbf{y^0}}} \\times \\frac{\\partial{\\mathbf{y^0}}}{\\partial\\mathbf{{v^0}}} \\times \\frac{\\partial{\\mathbf{v^0}}}{\\partial{\\mathbf{b}^0}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c43cc8-94ce-41aa-b21c-5bcae150876e",
   "metadata": {},
   "source": [
    "#### Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67db1ea3-b665-4ea4-9d3c-622db16c4a81",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial{\\mathbf{v}^1}}{\\partial{\\mathbf{y}^0}} = \\mathbf{W}^1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df67c127-ce71-4fae-95c9-be675587f82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "∂v1/∂y0 = \n",
      "[[0.5]\n",
      " [0.6]]\n"
     ]
    }
   ],
   "source": [
    "# ∂v1/∂y0\n",
    "dv1_dy0 = W1\n",
    "print(f'∂v1/∂y0 = \\n{np.around(dv1_dy0, DEC)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663dec7f-ec28-4ba0-86ae-d40585f0e0a0",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial{L}}{\\partial{\\mathbf{y^0}}} = \\frac{\\partial{L}}{\\partial{\\mathbf{\\hat{y}}}} \\times \\frac{\\partial{\\mathbf{\\hat{y}}}}{\\partial{\\mathbf{v^1}}} \\times \\frac{\\partial{\\mathbf{v^1}}}{\\partial{\\mathbf{y^0}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2913063e-b28b-4dd4-b66a-23ce43c048d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "∂L/∂y0 * = \n",
      "[[-0.0541]\n",
      " [-0.0649]]\n",
      "\n",
      "∂L/∂y01 = [-0.0541 -0.0649]\n"
     ]
    }
   ],
   "source": [
    "# ∂L/∂y0 = ∂L/∂y^ * ∂y^/∂v1 * ∂v1/∂y0\n",
    "# -----------------------------------\n",
    "dL_dy0_ = dL_dyhat * dyhat_dv1 * dv1_dy0\n",
    "print(f'∂L/∂y0 * = \\n{np.around(dL_dy0_, DEC)}')\n",
    "\n",
    "# Summing the contributions of layer 1\n",
    "dL_dy0 = dL_dy0_.sum(axis=1)\n",
    "print(f'\\n∂L/∂y01 = {np.around(dL_dy0, DEC)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fca30aa-c9b5-4c1b-bb5c-556654d7f9ac",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial{L}}{\\partial{\\mathbf{W}^0}} = \\frac{\\partial{L}}{\\partial{\\mathbf{y^0}}} \\times \\frac{\\partial{\\mathbf{y^0}}}{\\partial\\mathbf{{v^0}}} \\times \\frac{\\partial{\\mathbf{v^0}}}{\\partial{\\mathbf{W}^0}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9b169e3-7bba-4259-ad81-541b83b55eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "∂y0/∂v0 = [[0.24516332 0.24408866]]\n",
      "\n",
      "∂v0/∂W0 = \n",
      "[[[0.3]]]\n",
      "\n",
      "∂L/∂W0 = \n",
      "[[[-0.004  -0.0048]]]\n"
     ]
    }
   ],
   "source": [
    "# ∂y0/∂v0\n",
    "dy0_dv0 = sigmoid_grad(v0)\n",
    "print(f'∂y0/∂v0 = {dy0_dv0}')\n",
    "\n",
    "# ∂v0/∂W0\n",
    "dv0_dW0 = np.hstack([x[np.newaxis].T] * len(dy0_dv0))\n",
    "print(f'\\n∂v0/∂W0 = \\n{np.around(dv0_dW0, DEC)}')\n",
    "\n",
    "# ∂L/∂W0 = ∂L/∂y0 * ∂y0/∂v0 * ∂v0/∂yW0\n",
    "# ------------------------------------\n",
    "dL_dW0 = dL_dy0 * dy0_dv0 * dv0_dW0\n",
    "print(f'\\n∂L/∂W0 = \\n{np.around(dL_dW0, DEC)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6120e296-bdd4-4cdb-b8d9-3ce1fdded165",
   "metadata": {},
   "source": [
    "#### Bias\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{\\mathbf{b}^0}} = \\frac{\\partial{L}}{\\partial{\\mathbf{y^0}}} \\times \\frac{\\partial{\\mathbf{y^0}}}{\\partial\\mathbf{{v^0}}} \\times \\frac{\\partial{\\mathbf{v^0}}}{\\partial{\\mathbf{b}^0}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "029c1404-2525-4352-bd4c-e5b33c2416de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "∂L/∂b0 = [[-0.0133 -0.0158]]\n"
     ]
    }
   ],
   "source": [
    "# * ∂v1/∂b1 = 1\n",
    "# As the input for bias is fixed in 1, the derivatives are 1.\n",
    "\n",
    "# ∂L/∂b0 = ∂L/∂y0 * ∂y0/∂v0 \n",
    "# -----------------------------------\n",
    "dL_db0 = dL_dy0 * dy0_dv0 \n",
    "print(f'\\n∂L/∂b0 = {np.around(dL_db0, DEC)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0654b22a-d299-4d62-9640-116bea16cc80",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c032db14-cb4b-48eb-bc0d-973fd200c902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0 = \n",
      "[[[0.1 0.2]]]\n",
      "\n",
      "b0 = \n",
      "[[0.2501 0.2502]]\n",
      "\n",
      "W1 = \n",
      "[[[0.5006]\n",
      "  [0.6006]]\n",
      "\n",
      " [[0.5006]\n",
      "  [0.6006]]]\n",
      "\n",
      "b1 = \n",
      "[[0.3511]]\n"
     ]
    }
   ],
   "source": [
    "W1 = W1 - LEARNING_RATE * dL_dW1\n",
    "b1 = b1 - LEARNING_RATE * dL_db1\n",
    "\n",
    "W0 = W0 - LEARNING_RATE * dL_dW0\n",
    "b0 = b0 - LEARNING_RATE * dL_db0\n",
    "\n",
    "print(f'W0 = \\n{np.around(W0, DEC)}')\n",
    "print(f'\\nb0 = \\n{np.around(b0, DEC)}')\n",
    "\n",
    "print(f'\\nW1 = \\n{np.around(W1, DEC)}')\n",
    "print(f'\\nb1 = \\n{np.around(b1, DEC)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1351ea-9b55-42db-890c-7a1183699cd6",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "---\n",
    "* Rabindra Lamsal. A step by step forward pass and backpropagation example\n",
    "    * https://theneuralblog.com/forward-pass-backpropagation-example/\n",
    "* Matt Mazur. A Step by Step Backpropagation Example\n",
    "    * https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\n",
    "* Back-Propagation is very simple. Who made it Complicated ?\n",
    "    * https://medium.com/@14prakash/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c \n",
    "* Chapter 7: Artificial neural networks with Math.\n",
    "    * https://medium.com/deep-math-machine-learning-ai/chapter-7-artificial-neural-networks-with-math-bb711169481b\n",
    "* The Matrix Calculus You Need For Deep Learning\n",
    "    * http://explained.ai/matrix-calculus/index.html \n",
    "* How backpropagation works, and how you can use Python to build a neural network\n",
    "    * https://medium.freecodecamp.org/build-a-flexible-neural-network-with-backpropagation-in-python-acffeb7846d0 \n",
    "* All the Backpropagation derivatives\n",
    "    * https://medium.com/@pdquant/all-the-backpropagation-derivatives-d5275f727f60\n",
    "* Brent Scarff. Understanding Backpropagation. \n",
    "    * https://towardsdatascience.com/understanding-backpropagation-abcc509ca9d0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aeda18-7396-41a7-98b4-4ea7e4528ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
